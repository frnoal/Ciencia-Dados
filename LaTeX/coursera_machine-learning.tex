\textsf{Coursera} - Machine Learning
Professor: Geoffrey Hinton - University of Toronto
% Aluno: Filipe Ronald Noal\\\texttt{filipe.ronald@gmail.com}

\def\r          {`\textsf{R}'}%
\def\octave     {\textsf{GNU Oc\-ta\-ve}}%


\begin{abstract}
Welcome to Neural Networks for Machine Learning! You’re joining thousands of learners currently enrolled in the course. I'm excited to have you in the class and look forward to your contributions to the learning community.

To begin, I recommend taking a few minutes to explore the course site. Review the material we’ll cover each week, and preview the assignments you’ll need to complete to pass the course. Click Discussions to see forums where you can discuss the course material with fellow students taking the class.

If you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center.

Good luck as you get started, and I hope you enjoy the course!
\end{abstract}


%%%%%%%%%%
\section{Week I}


%%%%%%%%%%
\subsection{Syllabus and Course Logistics}

Welcome to the course!

There are 16 modules (``lectures'') and each module will be divided into about five short videos. In each module there will be a quiz that counts towards your final grade.

The modules are broken down into the following:

\begin{nitemize}
\item Lecture 1: Introduction
\item Lecture 2: The Perceptron learning procedure
\item Lecture 3: The backpropagation learning procedure
\item Lecture 4: Learning feature vectors for words
\item Lecture 5: Object recognition with neural nets
\item Lecture 6: Optimization: How to make the learning go faster
\item Lecture 7: Recurrent neural networks
\item Lecture 8: More recurrent neural networks
\item Lecture 9: Ways to make neural networks generalize better
\item Lecture 10: Combining multiple neural networks to improve generalization
\item Lecture 11: Hopfield nets and Boltzmann machines
\item Lecture 12: Restricted Boltzmann machines (RBMs)
\item Lecture 13: Stacking RBMs to make Deep Belief Nets
\item Lecture 14: Deep neural nets with generative pre-training
\item Lecture 15: Modeling hierarchical structure with neural nets
\item Lecture 16: Recent applications of deep neural nets (optional videos)
\end{nitemize}

\paragraph{The two module quizzes}

Each module, there will be two quizzes (one per lecture) that do count towards your final grade.


\paragraph{The programming assignments}

You will need to answer questions about the results produced by the programs and your answers will count towards your final grade.

The first programming assignment will be very simple. It is mainly intended to get you to download Octave and get used to using it (see the Octave installation link). We regret that we do not have the resources to support other languages, but if you have Matlab it should be simple to adapt the Octave code we provide. You will not need to submit any code.


\paragraph{Final test}

The final test will be 25\% of the final grade, the programming assignments will be 30\% and the weekly quizzes 45%.

Please remember that this course contains the same content presented on Coursera beginning in 2013. It is not a continuation or update of that original course


\paragraph{Help}

If you have a question about using the Coursera platform, your Coursera account, or any technical issues, please visit Coursera's Learner Help Center. Coursera regularly updates the Help Center with solutions to the most common learner issues. You'll also find a learner-to-learner Support Forum community where learners can post questions or read past responses to get immediate answers. If you still need help after troubleshooting, 24/7 chat is available or you can submit a support request to Coursera at the bottom of relevant resources.


%%%%%%%%%%
\subsection{Why do we need machine learning?}

\begin{multicols}{2}
\begin{footnotesize}
Hello. Welcome to the Coursera course on Neural Networks for Machine Learning. Before we get into the details of neural network learning algorithms, I want to talk a little bit about machine learning, why we need machine learning, the kinds of things we use it for, and show you some examples of what it can do. So the reason we need machine learning is that the sum problem, where it's very hard to write the programs, recognizing a three dimensional object for example. When it's from a novel viewpoint and new lighting additions in a cluttered scene is very hard to do. We don't know what program to write because we don't know how it's done in our brain. And even if we did know what program to write, it might be that it was a horrendously complicated program. 

Another example is, detecting a fraudulent credit card transaction, where there may not be any nice, simple rules that will tell you it's fraudulent. You really need to combine, a very large number of, not very reliable rules. And also, those rules change every time because people change the tricks they use for fraud. So, we need a complicated program that combines unreliable rules, and that we can change easily. 

The machine learning approach, is to say, instead of writing each program by hand for each specific task, for particular task, we collect a lot of examples, and specify the correct output for given input. A machine learning algorithm then takes these examples and produces a program that does the job. The program produced by the linear algorithm may look very different from the typical handwritten program. For example, it might contain millions of numbers about how you weight different kinds of evidence. If we do it right, the program should work for new cases just as well as the ones it's trained on. And if the data changes, we should be able to change the program runs very easily by retraining it on the new data. And now massive amounts for computation are cheaper that paying someone to write a program for a specific task, so we can afford big complicated machine learning programs to produce these stark task specific systems for us.

Some examples of the things that are best done by using a learning algorithm are recognizing patterns, so for example objects in real scenes, or the identities or expressions of people's faces, or spoken words. There's also recognizing anomalies. So, an unusual sequence of credit card transactions would be an anomaly. Another example of an anomaly would be an unusual pattern of sensor readings in a nuclear power plant. And you wouldn't really want to have to deal with those by doing supervised learning. Where you look at the ones that blow up, and see what, what caused them to blow up. You'd really like to recognize that something funny is happening without having any supervision signal. It's just not behaving in its normal way. And then this prediction. So, typically, predicting future stock prices or currency exchange rates or predicting which movies a person will like from knowing which other movies they like. And which movies a lot of other people liked.

So in this course I'm mean as a standard example for explaining a lot of the machine learning algorithms. This is done in a lot of science. In genetics for example, a lot of genetics is done on fruitflies. And the reason is they're convenient. They breed fast and a lot is already known about the genetics of fruit flies. The MNIST database of handwritten digits is the machine equivalent of fruitflies. It's publicly available. We can get machine learning algorithms to learn how to recognize these handwritten digits quite quickly, so it's easy to try lots of variations. And we know huge amounts about how well different machine learning methods do on MNIST. And in particular, the different machine learning methods were implemented by people who believed in them, so we can rely on those results. So for all those reasons, we're gonna use MNIST as our standard task.

Here's an example of some of the digits in MNIST. These are ones that were correctly recognized by neural net the first time it saw them. But the ones within the neural net wasn't very confident. And you could see why. I've arranged these digits in standard scan line order. So zeros, then ones, then twos and so on. If you look at a bunch of tubes like the onces in the green rectangle. You can see that if you knew they were 100 in digit you'd probably guess they were twos. But it's very hard to say what it is that makes them twos. Theres nothing simple that they all have in common. In particular if you try and overlay one on another you'll see it doesn't fit. And even if you skew it a bit, it's very hard to make them overlay on each other. So a template isn't going to do the job. An in particular template is going to be very hard to find that will fit those twos in the green box and would also fit the things in the red boxes. So that's one thing that makes recognizing handwritten digits a good task for machine learning. Now, I don't want you to think that's the only thing we can do. It's a relatively simple for our machine learning system to do now. And to motivate the rest of the course, I want to show you some examples of much more difficult things. So we now have neural nets with approaching a hundred million parameters in them, that can recognize a thousand different object classes in 1.3 million high resolution training images got from the web. So, there was a competition in 2010, and the best system got 47 percent error rate if you look at its first choice, and 25 percent error rate if you say it got it right if it was in its top five choices, which isn't bad for 1,000 different objects.

Jitendra Malik who's an eminent neural net skeptic, and a leading computer vision researcher, has said that this competition is a good test of whether deep neural networks can work well for object recognition. And a very deep neural network can now do considerably better than the thing that won the competition. It can get less than 40 percent error, for its first choice, and less than twenty percent error for its top five choices. I'll describe that in much more detail in lecture five.

Here's some examples of the kinds of images you have to recognize. These images from the test set that he's never seen before. And below the examples, I'm showing you what the neural net thought the right answer was. Where the length of the horizontal bar is how confident it was, and the correct answer is in red. So if you look in the middle, it correctly identified that as a snow plow. But you can see that its other choices are fairly sensible. It does look a little bit like a drilling platform. And if you look at its third choice, a lifeboat, it actually looks very like a lifeboat. You can see the flag on the front of the boat and the bridge of the boat and the flag at the back, and the high surf in the background. So its, its errors tell you a lot about how it's doing it and they're very plausible errors. If you look on the left, it gets it wrong possibly because the beak of the bird is missing and cuz the feathers of the bird look very like the wet fur of an otter. But it gets it in its top five, and it does better than me. I wouldn't know if that was a quail or a ruffed grouse or a partridge. If you look on the right, it gets it completely wrong. It a guillotine, you can why it says that. You can possibly see why it says orangutan, because of the sort of jungle looking background and something orange in the middle. But it fails to get the right answer. It can, however, deal with a wide range of different objects. If you look on the left, I would have said microwave as my first answer. The labels aren't very systematic. So actually, the correct answer there is electric range. And it does get it in its top five. In the middle, it's getting a turnstile, which is a distributed object. It does, can't, it can do more than just recognize compact things. And it can also deal with pictures, as well as real scenes, like the bulletproof vest. And it makes some very cool errors. If you look at the image on the left, that's an earphone. It doesn't get anything, like an earphone. But if you look at this fourth batch, it thinks it's an ant. And for you to think that's crazy. But then if you look at it carefully, you can see it's a view of an ant from underneath. The eyes are looking down at you, and you can see the antennae behind it. It's not the kind of view of an ant you'd like to have if you were a green fly. If you look at the one on the right, it doesn't get the right answer. But all of its answers are, cylindrical objects.

Another task that neural nets are now very good at, is speech recognition. Or at least part of a speech recognition system. So speech recognition systems have several stages. First they pre-process the sound wave, to get a vector of acoustic coefficients, for each ten milliseconds of sound wave. And so they get 100 of those actors per second. They then take a few adjacent vectors of acoustic coefficients, and they need to place bets on which part of which phoneme is being spoken. So they look at this little window and they say, in the middle of this window, what do I think the phoneme is, and which part of the phoneme is it? And a good speech recognition system will have many alternative models for a phoneme. And each model, it might have three different parts. So it might have many thousands of alternative fragments that it thinks this might be. And you have to place bets on all those thousands of alternatives. And then once you place those bets you have a decoding stage that does the best job it can of using plausible bets, but piecing them together into a sequence of bets that corresponds to the kinds of things that people say.

Currently, deep neural networks pioneered by George Dahl and Abdel-rahman Mohammed of the University of Toronto are doing better than previous machine learning methods for the acoustic model, and they're now beginning to be used in practical systems. So, Dahl and Mohammed, developed a system, that uses many layers of, binary neurons, to, take some acoustic frames, and make bets about the labels. They were doing it on a fairly small database and then used 183 alternative labels. And to get their system to work well, they did some pre-training, which will be described in the second half of the course. After standard post processing, they got 20.7 percent error rate on a very standard benchmark, which is kind of like the NMIST for speech. The best previous result on that benchmark for speak independent recognition was 24.4\%. And a very experienced speech researcher at Microsoft research realized that, that was a big enough improvement, that probably this would change the way speech recognition systems were done. And indeed, it has. So, if you look at recent results from several different leading speech groups, Microsoft showed that this kind of deep neural network, when used as the acoustic model in the speech system. Reduced the error rate from 27.4 percent to 18.5\%, or alternatively, you could view it as reducing the amount of training data you needed from 2,000 hours down to 309 hours to get comparable performance. IBM which has the best system for one of the standard speech recognition tasks for large recovery speech recognition, showed that even it's very highly tuned system that was getting 18.8 percent can be beaten by one of these deep neural networks. And Google, fairly recently, trained a deep neural network on a large amount of speech, 5,800 hours. That was still much less than they trained their mixture model on. But even with much less data, it did a lot better than the technology they had before. So it reduced the error rate from sixteen percent to 12.3 percent and the error rate is still falling. And in the latest Android, if you do voice search, it's using one of these deep neurall networks in order to do very good speech recognition.
\end{footnotesize}
\end{multicols}


%%%%%%%%%%
\subsection{What are neural networks?}

\begin{multicols}{2}
\begin{footnotesize}
In this video, I'm gonna tell you a little bit about real neurons on the real brain which provide the inspiration for the artificial neural network that we're gonna learn about in this course. In most of the course, we won't talk much about real neurons but I wanted to give you a quick overview of the beginning.

There's several different reasons to study how networks of neurons can compute things.
\begin{nitemize}
\item The first is to understand how the brain actually works. You might think we could do that just by experiments on the brain. But it's very big and complicated, and it dies when you poke it around. And so we need to use computer simulations to help us understand what we're discovering in empirical studies.

\item The second is to understand the style of parallel computation, this inspired by the fact that the brain can compute with a big parallel network, a world of relatively slow neurons. If you can understand that style of parallel computation we might be able to make better parallel computers. It's very different from the way computation is done on a conventional serial processor. It should be very good for things that brains are good at like vision, and it should also be bad for things that brains are bad at by multiplying two numbers together. 

\item A third reason, which is the relevant one for this course, is to solve practical problems by using novel learning algorithms that were inspired by the brain. These algorithms can be very useful even if they're not actually how the brain works.
\end{nitemize}
So in most of this course we won't talk much about how the brain actually works. It's just used as a source of inspiration to tell us the big, parallel networks of neurons can compute very complicated things. 

I'm gonna talk more in this video though about how the brain actually works. A typical cortical neuron has a gross physical structure that consists of a cell body, and an axon where it sends messages to other neurons, and a denditric tree where it receives messages from other neurons. Where an axon from one neuron contacts a dendritic tree of another neuron, there's a structure called a synapse. And a spike of activity traveling along the axon, causes charge to be injected into the post synaptic neuron at a synapse. A neuron generates spikes when it's received enough charge in its dendritic tree to depolarize a part of the cell body called the axon hillock. And when that gets depolarized, the neuron sends a spike out along its axon. And the spike's just a wave of depolarization that travels along the axon. 

Synapses themselves have interesting structure. They contain little vesicles of transmitter chemical and when a spike arrives in the axon it causes these vesicles to migrate to the surface and be released into the synaptic cleft. There's several different kinds of transmitter chemical. There's one that implement positive weights and ones that implement negative weights. The transmitter molecules diffuse across the synaptic clef and bind to receptor molecules in the membrane of the post-synaptic neuron, and by binding to these big molecules in the membrane they change their shape, and that creates holes in the membrane. These holes are like specific ions to flow in or out of the post-synaptic neuron and that changes their state of depolarization. Synapses adapt, and that's what most of learning is, changing the effectiveness of a synapse. They can adapt by varying the number of vesicles that get released when a spike arrives. Or by varying the number of receptor molecules that are sensitive to the released transmitter molecules. Synapses are very slow compared with computer memory. But they have a lot of advantages over the random access memory on a computer, they're very small and very low power. And they can adapt. That's the most important property. They use locally available signals to change their strengths, and that's how we learn to perform complicated computations. The issue of course is how do they decide how to change their strength? What is the, what are the rules for how they should adapt.

So, all on one slide this is how the brain works. Each neuron receives inputs from other neurons. A few of the neurons receive inputs from the receptors. It's a large number of neurons, but only a small fraction of them. And, the neurons communicate with each other within in the cortex by sending these spikes of activity. The effective in input line on a neuron is controlled by synaptic weight, which can be positive or negative. And these synaptic weights adapt. And by adapting these weights the whole network learns to perform different kinds of computation. For example recognizing objects, understanding language, making plans, controlling the movements of your body. You have about ten to the eleven neurons, each of which has about ten to the four weights. So you probably ten to the fifteen or maybe only about ten to the fourteen synaptic weights. And a huge number of these weights, quite a large fraction of them, can affect the ongoing computation in a very small fraction of a second, in a few milliseconds. That's much better bandwidth to stored knowledge than even a modern workstation has.

One final point about the brain is that the cortex is modular, at least it learns to be modular. Different bits of the cortex end up doing different things. Genetically, the inputs from the senses go to different bits of the cortex. And that determines a lot about what they end up doing. If you damage the brain of an adult, local damage to the brain causes specific effects. Damage to one place might cause you to lose your ability to understand language. Damage to another place might cause you to lose your ability to recognize objects. We know a lot about how functions are located in the brain because when you use a part of the brain for doing something it requires energy, and so it demands more blood flow, and you can see the blood flow in a brain scanner. That allows you to see which bits of the brain you're using for particular tasks.

But the remarkable thing about cortex is it looks pretty much the same all over, and that strongly suggests that it's got a fairly flexible universal learning algorithm in it. That's also suggested by the fact that if you damage the brain early on, functions will relocate to other parts of the brain. So it's not genetically predetermined, at least not directly, which part of the brain will perform which function. There's convincing experiments on baby ferrets that show that if you cut off the input to the auditory cortex that comes from the ears, and instead, reroute the visual input to auditory cortex, then the auditory cortex that was destined to deal with sounds will actually learn to deal with visual input, and create neurons that look very like the neurons in the visual system. 

This suggest the cortex is made of general purpose stuff that has the ability to turn into special purpose hardware for particular tasks in response to experience. And that gives you a nice combination of, rapid parallel computation once you have learnt, plus flexibility, so you can put, you can learn new functions, so you are learning, to do the parallel computation. Its quiet like a FPGA, where you build standard parallel hardware, then after its built, you put in information that tells it what particular parallel computation to do. Conventional computers get their flexibility by having a stored sequential program. But this required very fast central processors to access the lines in the sequential program and perform long sequential computations. 
\end{footnotesize}
\end{multicols}


%%%%%%%%%%
\subsection{Some simple models of neurons}

\begin{multicols}{2}
\begin{footnotesize}
In this video, I'm going to describe some relatively simple models of neurons. I'll describe a number of different models starting with simple linear and threshold neurons, and then, describing slightly more complicated models. These are much simpler than real neurons, but they're still complicated enough, to allow us to make neural nets, that do some very interesting kinds of machine learning. In order to understand anything complicated, we have to idealize it. That is, we have to make simplifications that allow us to get a handle on how it might work. With atoms, for example, we simplify them as behaving like little solar systems. Idealization removes the complicated details that are not essential for understanding the main principles. It allows us to apply mathematics, and to make analogies to other familiar systems. And once we understand the basic principles, it's easy to add complexity, and make the model more faithful to reality.

Of course, we have to be careful when we idealize something, not to remove the thing that's giving it its main properties. It's often worth understanding models that are known to be wrong, as long as we don't forget they're wrong. So for example, a lot of work on neural networks uses neurons that communicate real values rather than discrete spikes of activity, and we know cortical neurons don't behave like that, but it's still worth understanding systems like that, and in practice they can be very useful for machine learning.

The first kind of neuron I want to tell you about is the simplest, it's a linear neuron. It's simple. It's computationally limited in what it can do. It may allow us to get insights into more complicated neurons. But it may be somewhat misleading. So in a linear neuron, the output $y$ is a function of a bias set in your run B and the sum of all its incoming connections of the activity on an input line $x$ times the weight $w$ on that line that's the synaptic weight on the input line. If you plot that as curve, then if you plot on the $x$-axis, the bias plus the weighted activities on the input line we get a straight line that goes through zero.

\begin{equation}
y = b + \sum_i x_i w_i
\label{eq:linear-neuron}
\end{equation}

Very different from linear neurons, are binary threshold neurons that were introduced by McCulloch and Pitts. They actually influenced Von Neumann when he was thinking about how to design a universal computer.

In a binary threshold neuron, you first compute a weighted sum of the inputs and then you send out a spike of activity if that weighted sum exceeds the threshold. McCulloch and Pitts thought that the spikes were like the truth values of propositions. So each neuron is combining the truth values it gets from other neurons to produce the truth value of its own. And that's like combining some propositions to compute the truth value of another proposition. At the time in the 1940's logic was the main paradigm for how the mind might work. Since then people thinking about how the brain computes have become much more interested in the idea the brain is combining lots of different sources of unreliable evidence. And so logic isn't such a good pardigm for what the brain's up to. For a binary threshold neuron, you can think of its input\slash output function as if the weighted input is above the threshold, it gives an output of one. Otherwise, it gives an output of zero.

There are actually two equivalent ways to write the equations for a binary threshold neuron. We can say that the total input $z$ is just the activities on the input lines $x$ times the weights $w$. And then the output $y$ is one if that $z$ is above the threshold and zero otherwise.

\begin{equation}
z = \sum_i x_i w_i
\qquad
y = \left\{
    \begin{array}{ll}
    1, & \mbox{ se } z\geq\theta \\
    0, & \mbox{ outro valor} \\
    \end{array}
    \right.
\label{eq:binary-threshold-formulation1}
\end{equation}

Alternatively, we could say that the total input includes a bias term $b$. So the total input is what comes in on the input lines $x$, times the weights $w$, plus this bias term $b$. And then we could say the output is one if that total input is above zero and is zero otherwise. And the equivalence is simply that the threshold in first formulation is equal to the negative of the bias in the second formulation $\theta = -b$.

\begin{equation}
z = b + \sum_i x_i w_i
\qquad
y = \left\{
    \begin{array}{ll}
    1, & \mbox{ se } z\geq0 \\
    0, & \mbox{ outro valor} \\
    \end{array}
    \right.
\label{eq:binary-threshold-formulation2}
\end{equation}


A kind of neuron that combines the properties of both linear neurons and binary threshold neurons is a rectified linear neuron. It first computes a linear weighted sum of its inputs, but then it gives an output that's a non-linear function of this weighted sum. So we compute $z$ in the same way as before.

\begin{equation}
z = b + \sum_i x_i w_i
\qquad
y = \left\{
    \begin{array}{ll}
    z, & \mbox{ se } z>0 \\
    0, & \mbox{ outro valor} \\
    \end{array}
    \right.
\label{eq:linear-treashold-neuron}
\end{equation}

If $z$ is below zero, we give an output of zero. Otherwise, we give an output that's equal to $z$. So above zero is linear, and at zero, it makes a hard decision. So the input/output curve looks like this. It's definitely not linear, but above zero it is linear. So with a neuron like this, we can get a lot of the nice properties of linear systems, when it's above zero. We can also get the ability to make decisions, at zero.

The neurons that we'll use a lot in this course, and are probably the commonest kinds of neurons to use in artificial neuron nets, are \emph{sigmoid neurons}. They give a real valued output that is a smooth and bounded function of their total input. It's typical to use the logistic function, where the total input is computed as before, as a bias plus what comes in on the input lines, weighted. The output for a logistic neuron is:

\begin{equation}
z = b + \sum_i x_i w_i
\qquad
y = {1 \over 1+e^{-z}}
\label{eq:sigmoid-neuron}
\end{equation}

If you think about that, if the total input's big and positive. $e$ to the minus a big positive number is zero. And so, the output will be 1. If the total input's big and negative, $e$ to the minus a big negative number is a large number, and so the output will be zero. So the input output function looks like this. When, the total input's zero, e to the minus zero is 1, so the output's a half. And the nice thing about a sigmoid is it has, smooth derivatives. The derivatives change continuously. And, so they're nicely behaved, and they make it easy to do learning as we'll see in lecture three.

Finally the \emph{stochastic binary neurons}. They use just the same equations as logistic units. They compute their total input the same way and they use the logistic function to compute a real value which is the probability that they will output a spike. But then instead of outputting that probability as a real number they actually make a probabilistic decision, and so what they actually output is either a 1 or a zero. They're intrinsically random.

\begin{equation}
z = b + \sum_i x_i w_i
\qquad
p(s=1) = {1 \over 1+e^{-z}}
\label{eq:stochastic-binary-neuron}
\end{equation}

So they're treating the $p$ as the probability of producing a 1, not as a real number. Of course, if the input is very big and positive they will almost always produce a 1. If the input's big and negative they'll almost always produce a zero.

We can do a similar trick with rectified linear units. We can say that the output, there's real value that comes out of a rectified linear unit, if its input is above zero, is the rate of producing spikes. So that's deterministic. But once we've figured out these rate of producing spikes, the actual times at which spikes are produced is a random process. It's a Poisson process. So the rectified linear unit determines the rate, but intrinsic randomness in the unit determines when the spikes are actually produced.
\end{footnotesize}
\end{multicols}


%%%%%%%%%%
\subsection{A simple example of learning}

\begin{multicols}{2}
\begin{footnotesize}
In this video I am going to show you an example of machine learning. It is a very simple kind of NeuralNet and it is gonna be learning to recognize digits. And you gonna be able to see how the weights evolve, as we run a very simple learning algorithm. So we gonna look at the very simple learning algorithm for training a very simple network to recognize handwritten shapes. The network has two layers of neurons. It has got input neurons, whose activities represent the intensity of pixels, and output neurons, whose activities represent the class classes. 

What we'd like is that when we show a particular shape, the output neuron for that shape gets active. 

If a pixel is active what it does is it votes for particular shapes. Namely the shapes that contain that pixel. Each inked pixel can vote for several shapes, and the votes can have different intensities. The shape that gets the most vote wins. So we are assuming there is a competition between the output units---and that something I haven't explained yet will explain in a later lecture. 

So first, we need to decide how to display the weights. And it seems natural to write the weights on the connection between input unit and output unit. But, we are never able to see what was going on if we get that. We need a display in which we can see the values of thousands of weights. So the idea is for each output unit, we make a little map. And in that map we show the strength of connection coming from each input pixel in the location of that input pixel. And we show the strength of connection by using black and white blobs, whose area represents the magnitude, and whose color represents the sign.

So the initial weights that you see there are just small random weights. Now what we are gonna do is show that network some data and get it to learn weights that are better than the random weights. The way we are gonna look is when we show it an image, we are going to increment the weights from the active pixels in the image to the correct class if we just did that, the weights could get only bigger and eventually every class will get huge input whenever we show it to the image. So we need some way of keeping the weights under the control. What we gonna do is we will also gonna decrement the weights from the active pixels to whatever class the network guesses. So (??) training it to the right thing, rather than (??) currently has a tendency to do. If of course it does the right thing, then the increments we make, in the first step the learning rule will exactly cancel the decrements so nothing will change, which is what we want to. 

So, these are the initial weights. Now we are going to show you few hundred training examples and then look at the weights again. So now the weights have changed, they started to form regular patterns. And we show it a few more hundred examples. And the weights have changed small, and a few more hundred examples; and a few more hundred examples. Few more hundred, and now the weights are pretty much at their final values. I'll talk more in future lectures about precise details of the learning algorithm. But what you can see is the weights now look like the little templates from the shapes. If you look at the weights going into the 1 unit for example, they don't (??) little template for identifying ones. They are not quite templates. If you look at the weights going into the 9 unit, they don't have any positive weight below the half way line. That's because for telling the difference between 9s and 7s the weights below the half way line aren't much used. You have to tell the difference by deciding whether there is a loop at the top or horizontal bar at the top. And so, those output units are focused on that discrimination. 

One thing about this learning algorithm is because the network is so simple, it's unable to learn a very good way of discriminating shapes. 

What it learns is equivalent to having a little template for each shape. And then deciding the winner based on which shape has the template that overlapped most with the ink. The problem is that the weights in which handwritten digits vary are much too complicated to be captured by simple template matches of whole shapes. You have to model allowable variation for digits. By first extracting features and then looking arrangement of those features. 

So here is examples we have seen already. If you look at those 2's in green box, you can see there is no template that will fit all those well and will fail to fit that 3 in the red box there. So task simply can't be solved by a simple network like that. 

The network did the best it could but it can't solve this problem. 
\end{footnotesize}
\end{multicols}


%%%%%%%%%%
\subsection{Three types of learning}

\begin{multicols}{2}
\begin{footnotesize}
In this video, I'm gonna talk about three different types of machine learning: \emph{supervised learning}, \emph{reinforcement learning} and \emph{unsupervised learning}. Broadly speaking, the first half of the course will be about supervised learning. The second half of the course will be mainly about unsupervised learning, and reinforcement learning will not be covered in the course, because we can't cover everything.

Learning can be divided into three broad groups of algorithms.
In supervised learning, you're trying to predict an output when given an input vector, so it's very clear what the point of supervised learning is. In reinforcement level, you're trying to select actions or sequences of actions to maximize the rewards you get, and the rewards may only occur occasionally. In unsupervised learning you're trying to discover a good internal representation of the input and we'll come later to what that might mean.

\paragraph{Supervised learning} itself comes in two different flavors.
\begin{nitemize}
\item In regression, the target output is a real number or a whole vector of real numbers, such as the price of a stock in six months time, or the temperature at noon tomorrow. And the aim is to get as close as you can to the correct real number.
\item In classification, the target output is a class label. The simplest case is a choice between one and zero. Between positive and negative cases. But obviously, we can have multiple alternative labels as when we're classifying handwritten digits.
\end{nitemize}

Supervised learning works by initially selecting a model class, that is, a whole set of models that we're prepared to consider as candidates. You can think of a model class as a function that takes an input vector and some parameters and gives you an output $y$. So a model class is simply a way of mapping an input to an output using some numerical parameters $w$ and then we adjust these numerical parameters to make the mapping fit the supervised training data. What we mean by fit is minimizing the discrepancy between the target output on each training case and the actual output produced by a machine learning system. And an obvious measure of that discrepancy, if we're using real values as outputs, is the square difference between the output from our system $y$, and the correct output $t$, and we put in that one-half, so it cancels the two when we differentiate.

For classification you could use that measure, but there's other more sensiblbe measures which we'll come to later, and these more sensibile measures typically work better as well.

\paragraph{In reinforcement learning,} the outputs an actual sequence of actions, and you have to decide on those actions based on occasional rewards. The goal in selecting each action is to maximize the expected sum of the future rewards, and we typically use a discount factor so that you don't have to look too far in the future. We say that rewards far in the future don't count for as much as rewards that you get fairly quickly. Reinforcement learning is difficult. It's difficult because the rewards are typically delayed, so it's hard to know exactly which action was the wrong one in a long sequence of actions. It's also difficult because a scalar award, especially one that only occurs occasionally, does not supply much information, on which to base the changes in parameters. So typically, you can't learn millions of parameters using reinforcement learning, whereas supervised learning and unsupervised learning, you can. Typically, in reinforcement learning, you're trying to learn dozens of parameters or maybe 1,000 parameters, but not millions. In this course, we can't cover everything, and so we're not going to cover reinforcement learning, even though it's an important topic.

\paragraph{Unsupervised learning,} is going to be covered in the second half of the course. For about 40 years, the machine learning community basically ignored unsupervised learning except for one very limited form called clustering. In fact, they used definitions of machine learning that excluded it. So they defined machine learning, in some textbooks, as mapping from inputs to outputs. And many researchers thought that clustering was the only form of unsupervised learning. One reason for this is that it's hard to say what the aim of unsupervised learning is. One major aim is to get an internal representation of the input, that is useful for subsequent supervised or reinforcement learning. And the reason we might want to do that in two stages, is we don't want to use, for example, the payoffs from reinforcement learning, in order to set the parameters, for our visual system. So you can compute the distance to a surface by using the disparity between the images you get in your two eyes. But you don't want to learn to do that computation of distance by repeatedly stubbing your toe and adjusting the parameters in your visual system every time you stub your toe. That would involve stubbing your toe a very large number of times and there's much better ways to learn to fuse two images based purely on the information in the inputs.

Other goals for unsupervised learning are to provide compact, low dimensional representations of the input. So, high-dimensional inputs like images, typically, live on or near a low-dimensional manifold. Or several such manifolds in the case of the handwritten digits. What that means is, even if you have a million pixels, there aren't really a million degrees of freedom in what can happen. There may only be a few hundred degrees of freedom in what can happen. So what we want to do is move from a million pixels to a representation of those few hundred degrees of freedom which will be according to saying where we are on a manifold. Also we need to know which manifold we're on. A very limited form of this is principle commands analysis which is linear. It assumes that there's one manifold, and the manifold is a plane in the high dimensional space.

Another definition of unsupervised learning, or another goal for unsupervised learning, is to provide an economical representation for the input in terms of learned features. If, for example, we can represent the input in terms of binary features, that's typically economical cuz then it takes only one bit to say the state of a binary feature. Alternatively we could use a large number of real valued features but insist that for each input almost all of those features are exactly zero. In that case for each input we only need to represent a few real numbers and that's economical.

As I mentioned before, another definition of unsupervised learning or another goal of unsupervised learning is to find clusters in the input, and clustering could be viewed as a very sparse code, that is we have one feature per cluster and we insist that all the features except one are zero and that one feature has a value of one. So clustering is really just an extreme case of finding sparse features.
\end{footnotesize}
\end{multicols}


%%%%%%%%%%
\section{Week II}


%%%%%%%%%%
\subsection{Types of neural network architectures}

\begin{multicols}{2}
\begin{footnotesize}
In this video I'm going to describe various kinds of architectures for neural networks. What I mean by an architecture, is the way in which the neurons are connected together. By far the commonest type of architecture in practical applications is a feet forward neural network where the information comes into the imput units and flows in one direction through hidden layers until each reaches the output units.

A much more interesting kind architecture is a recurrent neural network in which information can flow round in cycles. These networks can remember information for a long time. They can exhibit all sorts of interesting oscillations but they are much more difficult to train in part because they are so much more complicated in what they can do. Recently, however, people have made a lot of progress in training recurrent neural networks, and they can now do some fairly impressive things.

The last kind of architecture that I'll describe is a symmetrically-connected network, one in which the weights are the same in both directions between two units. The commonest type of neural network in practical applications is a feed-forward neural network. This has some input units in the first layer at the bottom, some output units in the last layer at the top, and one or more layers of hidden units. If there's more than one layer of hidden units, we call them deep neural networks. These networks compute a series of transformations between their input and their output. So at each layer, you get a new representation of the input in which things that were similar in the previous layer may have become less similar, or things that were dissimilar in the previous layer may have become more similar. So in speech recognition, for example, we'd like the same thing said by different speakers to become more similar, and different things said by the same speaker to be less similar as we go up through the layers of the network. In order to achieve this, we need the activities of the neurons in each layer to be a non-linear function of the activities in the layer below. 

Recurrent neural networks are much more powerful than feed forward neural networks. They have directed cycles in the direct, in their connection graph. What this means is that if you start at a node or a neuron and you follow the arrows, you can sometimes get back to the neuron you started at. They can have very complicated dynamics, and this can make them very difficult to train. 

There's a lot of interest at present at finding efficient ways of training our current \emph{[inaudible]}, because they are so powerful if we can train them. They're also more biologically realistic. Recurrent neural networks with multiple hidden layers are really just a special case of a general recurrent neural network that has some of its hidden to hidden connections missing. 

Recurring networks are a very natural way to model sequential data.
So what we do is we have connections between hidden units. And the hidden units act like a network that's very deep in time. So at each time step the states of the hidden units determines the states of the hidden units of the next time step. One way in which they differ from feed-forward nets is that we use the same weights at every time step. So if you look at those red arrows where the hidden units are determining the next state of the hidden units, the weight matrix depicted by each red arrow is the same at each time step. They also get inputs at every time step and often give outputs at every time step, and those'll use the same weight matrices too. Recurrent networks have the ability to remember information in the hidden state for a long time. Unfortunately, it's quite hard to train them to use that ability. However, recent algorithms have been able to do that. 

So just to show you what recurrent neural nets can now do, I'm gonna show you a net designed by Ilya Sutskever. It's a special kind of recurrent neural net, slightly different from the kind in the diagram on the previous slide, and it's used to predict the next character in a sequence. So Ilya trained it on lots and lots of strings from English Wikipedia. It's seeing English characters and trying to predict the next English character. He actually used 86 different characters to allow for punctuation, and digits, and capital letters and so on. After you trained it, one way of seeing how well it can do is to see whether it assigns high probability to the next character that actually occurs. Another way of seeing what it can do is to get it to generate text. So what you do is you give it a string of characters and get it to predict probabilities for the next character. Then you pick the next character from that probability distribution. It's no use picking the most likely character. If you do that after a while it starts saying the United States of the United States of the United States of the United States. That tells you something about Wikipedia. But if you pick from the probability distribution, so if it says there's a one in 100 chance it was a Z, you pick a Z one time in 100, then you see much more about what it's learned. The next slide shows an example of the text that it generates, and it's interesting to notice how much is learned just by reading Wikipedia, and trying to predict the next character. So remember this text was generated one character at a time. Notice that it makes reasonable sensible sentences and they composed always entirely of real English words. Occasionally, it makes a non-word but they typically sensible ones. And notice that within a sentence, it has some thematic sentence. So the phrase, Several Irish intelligence agents is in the Mediterranean region, has problems but it's almost good English. Notice also the thing it says at the end, such that it is the blurring of appearing on any well-paid type of box printer. There's a certain sort of thematic thing there about appearance and printing, and the syntax is pretty good. And remember, that's one character at a time.

\begin{quote}
\textit{In 1974 Northern Denver had been
overshadowed by CNL, and several Irish
intelligence agencies in the Mediterranean
region. However, on the Victoria, Kings
Hebrew stated that Charles decided to
escape during an alliance. The mansion
house was completed in 1882, the second in
its bridge are omitted, while closing is the
proton reticulum composed below it aims,
such that it is the blurring of appearing on any
well-paid type of box printer.}
\end{quote}


Quite different for a current nets, symmetrically connected networks.
In these the connections between units have the same weight in both directions. John Hopfield and others realized that symmetric networks are much easier to analyze than recurrent networks. This is mainly because they're more restricted in what they can do, and that's because they obey an energy function. So they come, for example, model cycles. You can't get back to where you started in one of these symmetric networks. 
\end{footnotesize}
\end{multicols}

