\textsf{Coursera} - Machine Learning
Professor: Geoffrey Hinton - University of Toronto
% Aluno: Filipe Ronald Noal\\\texttt{filipe.ronald@gmail.com}

\def\r          {`\textsf{R}'}%
\def\octave     {\textsf{GNU Oc\-ta\-ve}}%


\begin{abstract}
Welcome to Neural Networks for Machine Learning! You’re joining thousands of learners currently enrolled in the course. I'm excited to have you in the class and look forward to your contributions to the learning community.

To begin, I recommend taking a few minutes to explore the course site. Review the material we’ll cover each week, and preview the assignments you’ll need to complete to pass the course. Click Discussions to see forums where you can discuss the course material with fellow students taking the class.

If you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center.

Good luck as you get started, and I hope you enjoy the course!
\end{abstract}


%%%%%%%%%%
\section{Week I}


%%%%%%%%%%
\subsection{Syllabus and Course Logistics}

Welcome to the course!

There are 16 modules (``lectures'') and each module will be divided into about five short videos. In each module there will be a quiz that counts towards your final grade.

The modules are broken down into the following:

\begin{nitemize}
\item Lecture 1: Introduction
\item Lecture 2: The Perceptron learning procedure
\item Lecture 3: The backpropagation learning procedure
\item Lecture 4: Learning feature vectors for words
\item Lecture 5: Object recognition with neural nets
\item Lecture 6: Optimization: How to make the learning go faster
\item Lecture 7: Recurrent neural networks
\item Lecture 8: More recurrent neural networks
\item Lecture 9: Ways to make neural networks generalize better
\item Lecture 10: Combining multiple neural networks to improve generalization
\item Lecture 11: Hopfield nets and Boltzmann machines
\item Lecture 12: Restricted Boltzmann machines (RBMs)
\item Lecture 13: Stacking RBMs to make Deep Belief Nets
\item Lecture 14: Deep neural nets with generative pre-training
\item Lecture 15: Modeling hierarchical structure with neural nets
\item Lecture 16: Recent applications of deep neural nets (optional videos)
\end{nitemize}

\paragraph{The two module quizzes}

Each module, there will be two quizzes (one per lecture) that do count towards your final grade.


\paragraph{The programming assignments}

You will need to answer questions about the results produced by the programs and your answers will count towards your final grade.

The first programming assignment will be very simple. It is mainly intended to get you to download Octave and get used to using it (see the Octave installation link). We regret that we do not have the resources to support other languages, but if you have Matlab it should be simple to adapt the Octave code we provide. You will not need to submit any code.


\paragraph{Final test}

The final test will be 25\% of the final grade, the programming assignments will be 30\% and the weekly quizzes 45%.

Please remember that this course contains the same content presented on Coursera beginning in 2013. It is not a continuation or update of that original course


\paragraph{Help}

If you have a question about using the Coursera platform, your Coursera account, or any technical issues, please visit Coursera's Learner Help Center. Coursera regularly updates the Help Center with solutions to the most common learner issues. You'll also find a learner-to-learner Support Forum community where learners can post questions or read past responses to get immediate answers. If you still need help after troubleshooting, 24/7 chat is available or you can submit a support request to Coursera at the bottom of relevant resources.


%%%%%%%%%%
\subsection{Why do we need machine learning?}

\begin{multicols}{2}
\begin{footnotesize}
Hello. Welcome to the Coursera course on Neural Networks for Machine Learning. Before we get into the details of neural network learning algorithms, I want to talk a little bit about machine learning, why we need machine learning, the kinds of things we use it for, and show you some examples of what it can do. So the reason we need machine learning is that the sum problem, where it's very hard to write the programs, recognizing a three dimensional object for example. When it's from a novel viewpoint and new lighting additions in a cluttered scene is very hard to do. We don't know what program to write because we don't know how it's done in our brain. And even if we did know what program to write, it might be that it was a horrendously complicated program. 

Another example is, detecting a fraudulent credit card transaction, where there may not be any nice, simple rules that will tell you it's fraudulent. You really need to combine, a very large number of, not very reliable rules. And also, those rules change every time because people change the tricks they use for fraud. So, we need a complicated program that combines unreliable rules, and that we can change easily. 

The machine learning approach, is to say, instead of writing each program by hand for each specific task, for particular task, we collect a lot of examples, and specify the correct output for given input. A machine learning algorithm then takes these examples and produces a program that does the job. The program produced by the linear algorithm may look very different from the typical handwritten program. For example, it might contain millions of numbers about how you weight different kinds of evidence. If we do it right, the program should work for new cases just as well as the ones it's trained on. And if the data changes, we should be able to change the program runs very easily by retraining it on the new data. And now massive amounts for computation are cheaper that paying someone to write a program for a specific task, so we can afford big complicated machine learning programs to produce these stark task specific systems for us.

Some examples of the things that are best done by using a learning algorithm are recognizing patterns, so for example objects in real scenes, or the identities or expressions of people's faces, or spoken words. There's also recognizing anomalies. So, an unusual sequence of credit card transactions would be an anomaly. Another example of an anomaly would be an unusual pattern of sensor readings in a nuclear power plant. And you wouldn't really want to have to deal with those by doing supervised learning. Where you look at the ones that blow up, and see what, what caused them to blow up. You'd really like to recognize that something funny is happening without having any supervision signal. It's just not behaving in its normal way. And then this prediction. So, typically, predicting future stock prices or currency exchange rates or predicting which movies a person will like from knowing which other movies they like. And which movies a lot of other people liked.

So in this course I'm mean as a standard example for explaining a lot of the machine learning algorithms. This is done in a lot of science. In genetics for example, a lot of genetics is done on fruitflies. And the reason is they're convenient. They breed fast and a lot is already known about the genetics of fruit flies. The MNIST database of handwritten digits is the machine equivalent of fruitflies. It's publicly available. We can get machine learning algorithms to learn how to recognize these handwritten digits quite quickly, so it's easy to try lots of variations. And we know huge amounts about how well different machine learning methods do on MNIST. And in particular, the different machine learning methods were implemented by people who believed in them, so we can rely on those results. So for all those reasons, we're gonna use MNIST as our standard task.

Here's an example of some of the digits in MNIST. These are ones that were correctly recognized by neural net the first time it saw them. But the ones within the neural net wasn't very confident. And you could see why. I've arranged these digits in standard scan line order. So zeros, then ones, then twos and so on. If you look at a bunch of tubes like the onces in the green rectangle. You can see that if you knew they were 100 in digit you'd probably guess they were twos. But it's very hard to say what it is that makes them twos. Theres nothing simple that they all have in common. In particular if you try and overlay one on another you'll see it doesn't fit. And even if you skew it a bit, it's very hard to make them overlay on each other. So a template isn't going to do the job. An in particular template is going to be very hard to find that will fit those twos in the green box and would also fit the things in the red boxes. So that's one thing that makes recognizing handwritten digits a good task for machine learning. Now, I don't want you to think that's the only thing we can do. It's a relatively simple for our machine learning system to do now. And to motivate the rest of the course, I want to show you some examples of much more difficult things. So we now have neural nets with approaching a hundred million parameters in them, that can recognize a thousand different object classes in 1.3 million high resolution training images got from the web. So, there was a competition in 2010, and the best system got 47 percent error rate if you look at its first choice, and 25 percent error rate if you say it got it right if it was in its top five choices, which isn't bad for 1,000 different objects.

Jitendra Malik who's an eminent neural net skeptic, and a leading computer vision researcher, has said that this competition is a good test of whether deep neural networks can work well for object recognition. And a very deep neural network can now do considerably better than the thing that won the competition. It can get less than 40 percent error, for its first choice, and less than twenty percent error for its top five choices. I'll describe that in much more detail in lecture five.

Here's some examples of the kinds of images you have to recognize. These images from the test set that he's never seen before. And below the examples, I'm showing you what the neural net thought the right answer was. Where the length of the horizontal bar is how confident it was, and the correct answer is in red. So if you look in the middle, it correctly identified that as a snow plow. But you can see that its other choices are fairly sensible. It does look a little bit like a drilling platform. And if you look at its third choice, a lifeboat, it actually looks very like a lifeboat. You can see the flag on the front of the boat and the bridge of the boat and the flag at the back, and the high surf in the background. So its, its errors tell you a lot about how it's doing it and they're very plausible errors. If you look on the left, it gets it wrong possibly because the beak of the bird is missing and cuz the feathers of the bird look very like the wet fur of an otter. But it gets it in its top five, and it does better than me. I wouldn't know if that was a quail or a ruffed grouse or a partridge. If you look on the right, it gets it completely wrong. It a guillotine, you can why it says that. You can possibly see why it says orangutan, because of the sort of jungle looking background and something orange in the middle. But it fails to get the right answer. It can, however, deal with a wide range of different objects. If you look on the left, I would have said microwave as my first answer. The labels aren't very systematic. So actually, the correct answer there is electric range. And it does get it in its top five. In the middle, it's getting a turnstile, which is a distributed object. It does, can't, it can do more than just recognize compact things. And it can also deal with pictures, as well as real scenes, like the bulletproof vest. And it makes some very cool errors. If you look at the image on the left, that's an earphone. It doesn't get anything, like an earphone. But if you look at this fourth batch, it thinks it's an ant. And for you to think that's crazy. But then if you look at it carefully, you can see it's a view of an ant from underneath. The eyes are looking down at you, and you can see the antennae behind it. It's not the kind of view of an ant you'd like to have if you were a green fly. If you look at the one on the right, it doesn't get the right answer. But all of its answers are, cylindrical objects.

Another task that neural nets are now very good at, is speech recognition. Or at least part of a speech recognition system. So speech recognition systems have several stages. First they pre-process the sound wave, to get a vector of acoustic coefficients, for each ten milliseconds of sound wave. And so they get 100 of those actors per second. They then take a few adjacent vectors of acoustic coefficients, and they need to place bets on which part of which phoneme is being spoken. So they look at this little window and they say, in the middle of this window, what do I think the phoneme is, and which part of the phoneme is it? And a good speech recognition system will have many alternative models for a phoneme. And each model, it might have three different parts. So it might have many thousands of alternative fragments that it thinks this might be. And you have to place bets on all those thousands of alternatives. And then once you place those bets you have a decoding stage that does the best job it can of using plausible bets, but piecing them together into a sequence of bets that corresponds to the kinds of things that people say.

Currently, deep neural networks pioneered by George Dahl and Abdel-rahman Mohammed of the University of Toronto are doing better than previous machine learning methods for the acoustic model, and they're now beginning to be used in practical systems. So, Dahl and Mohammed, developed a system, that uses many layers of, binary neurons, to, take some acoustic frames, and make bets about the labels. They were doing it on a fairly small database and then used 183 alternative labels. And to get their system to work well, they did some pre-training, which will be described in the second half of the course. After standard post processing, they got 20.7 percent error rate on a very standard benchmark, which is kind of like the NMIST for speech. The best previous result on that benchmark for speak independent recognition was 24.4\%. And a very experienced speech researcher at Microsoft research realized that, that was a big enough improvement, that probably this would change the way speech recognition systems were done. And indeed, it has. So, if you look at recent results from several different leading speech groups, Microsoft showed that this kind of deep neural network, when used as the acoustic model in the speech system. Reduced the error rate from 27.4 percent to 18.5\%, or alternatively, you could view it as reducing the amount of training data you needed from 2,000 hours down to 309 hours to get comparable performance. IBM which has the best system for one of the standard speech recognition tasks for large recovery speech recognition, showed that even it's very highly tuned system that was getting 18.8 percent can be beaten by one of these deep neural networks. And Google, fairly recently, trained a deep neural network on a large amount of speech, 5,800 hours. That was still much less than they trained their mixture model on. But even with much less data, it did a lot better than the technology they had before. So it reduced the error rate from sixteen percent to 12.3 percent and the error rate is still falling. And in the latest Android, if you do voice search, it's using one of these deep neurall networks in order to do very good speech recognition.
\end{footnotesize}
\end{multicols}


%%%%%%%%%%
\subsection{What are neural networks?}

\begin{multicols}{2}
\begin{footnotesize}
In this video, I'm gonna tell you a little bit about real neurons on the real brain which provide the inspiration for the artificial neural network that we're gonna learn about in this course. In most of the course, we won't talk much about real neurons but I wanted to give you a quick overview of the beginning.

There's several different reasons to study how networks of neurons can compute things.
\begin{nitemize}
\item The first is to understand how the brain actually works. You might think we could do that just by experiments on the brain. But it's very big and complicated, and it dies when you poke it around. And so we need to use computer simulations to help us understand what we're discovering in empirical studies.

\item The second is to understand the style of parallel computation, this inspired by the fact that the brain can compute with a big parallel network, a world of relatively slow neurons. If you can understand that style of parallel computation we might be able to make better parallel computers. It's very different from the way computation is done on a conventional serial processor. It should be very good for things that brains are good at like vision, and it should also be bad for things that brains are bad at by multiplying two numbers together. 

\item A third reason, which is the relevant one for this course, is to solve practical problems by using novel learning algorithms that were inspired by the brain. These algorithms can be very useful even if they're not actually how the brain works.
\end{nitemize}
So in most of this course we won't talk much about how the brain actually works. It's just used as a source of inspiration to tell us the big, parallel networks of neurons can compute very complicated things. 

I'm gonna talk more in this video though about how the brain actually works. A typical cortical neuron has a gross physical structure that consists of a cell body, and an axon where it sends messages to other neurons, and a denditric tree where it receives messages from other neurons. Where an axon from one neuron contacts a dendritic tree of another neuron, there's a structure called a synapse. And a spike of activity traveling along the axon, causes charge to be injected into the post synaptic neuron at a synapse. A neuron generates spikes when it's received enough charge in its dendritic tree to depolarize a part of the cell body called the axon hillock. And when that gets depolarized, the neuron sends a spike out along its axon. And the spike's just a wave of depolarization that travels along the axon. 

Synapses themselves have interesting structure. They contain little vesicles of transmitter chemical and when a spike arrives in the axon it causes these vesicles to migrate to the surface and be released into the synaptic cleft. There's several different kinds of transmitter chemical. There's one that implement positive weights and ones that implement negative weights. The transmitter molecules diffuse across the synaptic clef and bind to receptor molecules in the membrane of the post-synaptic neuron, and by binding to these big molecules in the membrane they change their shape, and that creates holes in the membrane. These holes are like specific ions to flow in or out of the post-synaptic neuron and that changes their state of depolarization. Synapses adapt, and that's what most of learning is, changing the effectiveness of a synapse. They can adapt by varying the number of vesicles that get released when a spike arrives. Or by varying the number of receptor molecules that are sensitive to the released transmitter molecules. Synapses are very slow compared with computer memory. But they have a lot of advantages over the random access memory on a computer, they're very small and very low power. And they can adapt. That's the most important property. They use locally available signals to change their strengths, and that's how we learn to perform complicated computations. The issue of course is how do they decide how to change their strength? What is the, what are the rules for how they should adapt.

So, all on one slide this is how the brain works. Each neuron receives inputs from other neurons. A few of the neurons receive inputs from the receptors. It's a large number of neurons, but only a small fraction of them. And, the neurons communicate with each other within in the cortex by sending these spikes of activity. The effective in input line on a neuron is controlled by synaptic weight, which can be positive or negative. And these synaptic weights adapt. And by adapting these weights the whole network learns to perform different kinds of computation. For example recognizing objects, understanding language, making plans, controlling the movements of your body. You have about ten to the eleven neurons, each of which has about ten to the four weights. So you probably ten to the fifteen or maybe only about ten to the fourteen synaptic weights. And a huge number of these weights, quite a large fraction of them, can affect the ongoing computation in a very small fraction of a second, in a few milliseconds. That's much better bandwidth to stored knowledge than even a modern workstation has.

One final point about the brain is that the cortex is modular, at least it learns to be modular. Different bits of the cortex end up doing different things. Genetically, the inputs from the senses go to different bits of the cortex. And that determines a lot about what they end up doing. If you damage the brain of an adult, local damage to the brain causes specific effects. Damage to one place might cause you to lose your ability to understand language. Damage to another place might cause you to lose your ability to recognize objects. We know a lot about how functions are located in the brain because when you use a part of the brain for doing something it requires energy, and so it demands more blood flow, and you can see the blood flow in a brain scanner. That allows you to see which bits of the brain you're using for particular tasks.

But the remarkable thing about cortex is it looks pretty much the same all over, and that strongly suggests that it's got a fairly flexible universal learning algorithm in it. That's also suggested by the fact that if you damage the brain early on, functions will relocate to other parts of the brain. So it's not genetically predetermined, at least not directly, which part of the brain will perform which function. There's convincing experiments on baby ferrets that show that if you cut off the input to the auditory cortex that comes from the ears, and instead, reroute the visual input to auditory cortex, then the auditory cortex that was destined to deal with sounds will actually learn to deal with visual input, and create neurons that look very like the neurons in the visual system. 

This suggest the cortex is made of general purpose stuff that has the ability to turn into special purpose hardware for particular tasks in response to experience. And that gives you a nice combination of, rapid parallel computation once you have learnt, plus flexibility, so you can put, you can learn new functions, so you are learning, to do the parallel computation. Its quiet like a FPGA, where you build standard parallel hardware, then after its built, you put in information that tells it what particular parallel computation to do. Conventional computers get their flexibility by having a stored sequential program. But this required very fast central processors to access the lines in the sequential program and perform long sequential computations. 
\end{footnotesize}
\end{multicols}


%%%%%%%%%%
\subsection{Some simple models of neurons}

\begin{multicols}{2}
\begin{footnotesize}
In this video, I'm going to describe some relatively simple models of neurons. I'll describe a number of different models starting with simple linear and threshold neurons, and then, describing slightly more complicated models. These are much simpler than real neurons, but they're still complicated enough, to allow us to make neural nets, that do some very interesting kinds of machine learning. In order to understand anything complicated, we have to idealize it. That is, we have to make simplifications that allow us to get a handle on how it might work. With atoms, for example, we simplify them as behaving like little solar systems. Idealization removes the complicated details that are not essential for understanding the main principles. It allows us to apply mathematics, and to make analogies to other familiar systems. And once we understand the basic principles, it's easy to add complexity, and make the model more faithful to reality.

Of course, we have to be careful when we idealize something, not to remove the thing that's giving it its main properties. It's often worth understanding models that are known to be wrong, as long as we don't forget they're wrong. So for example, a lot of work on neural networks uses neurons that communicate real values rather than discrete spikes of activity, and we know cortical neurons don't behave like that, but it's still worth understanding systems like that, and in practice they can be very useful for machine learning.

The first kind of neuron I want to tell you about is the simplest, it's a linear neuron. It's simple. It's computationally limited in what it can do. It may allow us to get insights into more complicated neurons. But it may be somewhat misleading. So in a linear neuron, the output $y$ is a function of a bias set in your run B and the sum of all its incoming connections of the activity on an input line $x$ times the weight $w$ on that line that's the synaptic weight on the input line. If you plot that as curve, then if you plot on the $x$-axis, the bias plus the weighted activities on the input line we get a straight line that goes through zero.

\begin{equation}
y = b + \sum_i x_i w_i
\label{eq:linear-neuron}
\end{equation}

Very different from linear neurons, are binary threshold neurons that were introduced by McCulloch and Pitts. They actually influenced Von Neumann when he was thinking about how to design a universal computer.

In a binary threshold neuron, you first compute a weighted sum of the inputs and then you send out a spike of activity if that weighted sum exceeds the threshold. McCulloch and Pitts thought that the spikes were like the truth values of propositions. So each neuron is combining the truth values it gets from other neurons to produce the truth value of its own. And that's like combining some propositions to compute the truth value of another proposition. At the time in the 1940's logic was the main paradigm for how the mind might work. Since then people thinking about how the brain computes have become much more interested in the idea the brain is combining lots of different sources of unreliable evidence. And so logic isn't such a good pardigm for what the brain's up to. For a binary threshold neuron, you can think of its input\slash output function as if the weighted input is above the threshold, it gives an output of one. Otherwise, it gives an output of zero.

There are actually two equivalent ways to write the equations for a binary threshold neuron. We can say that the total input $z$ is just the activities on the input lines $x$ times the weights $w$. And then the output $y$ is one if that $z$ is above the threshold and zero otherwise.

\begin{equation}
z = \sum_i x_i w_i
\qquad
y = \left\{
    \begin{array}{ll}
    1, & \mbox{ se } z\geq\theta \\
    0, & \mbox{ outro valor} \\
    \end{array}
    \right.
\label{eq:binary-threshold-formulation1}
\end{equation}

Alternatively, we could say that the total input includes a bias term $b$. So the total input is what comes in on the input lines $x$, times the weights $w$, plus this bias term $b$. And then we could say the output is one if that total input is above zero and is zero otherwise. And the equivalence is simply that the threshold in first formulation is equal to the negative of the bias in the second formulation $\theta = -b$.

\begin{equation}
z = b + \sum_i x_i w_i
\qquad
y = \left\{
    \begin{array}{ll}
    1, & \mbox{ se } z\geq0 \\
    0, & \mbox{ outro valor} \\
    \end{array}
    \right.
\label{eq:binary-threshold-formulation2}
\end{equation}


PAREI AQUI


A kind of neuron that combines the properties of both linear neurons and binary threshold neurons is a rectified linear neuron. It first computes a linear weighted sum of its inputs, but then it gives an output that's a non-linear function of this weighted sum. So we compute Z in the same way as before. If z is below zero, we give an output of zero. Otherwise, we give an output that's equal to z. So above zero is linear, and at zero, it makes a hard decision. So the input/output curve looks like this. It's definitely not linear, but above zero it is linear. So with a neuron like this, we can get a lot of the nice properties of linear systems, when it's above zero. We can also get the ability to make decisions, at zero.

The neurons that we'll use a lot in this course, and are probably the commonest kinds of neurons to use in artificial neuron [inaudible], are sigmoid neurons. They give a real valued output that is a smooth and bounded function of their total input. It's typical to use the logistic function, where the total input is computed as before, as a bias plus what comes in on the input lines, weighted. The output for a logistic neuron is one over one plus e to the minus, the total input. If you think about that, if the total input's big and positive. E to the minus a big positive number is zero. And so, the output will be one. If the total input's big and negative, E to the minus a big negative number is a large number, and so the output will be zero. So the input output function looks like this. When, the total input's zero, e to the minus zero is one, so the output's a half. And the nice thing about a sigmoid is it has, smooth derivatives. The derivatives, change continuously. And, so they're nicely behaved, and they make it easy to do learning as we'll see in lecture three. Finally the stochastic binary neurons. They use just the same equations as logistic units. They compute their total input the same way and they use the logistic function to compute a real value which is the probability that they will output a spike. But then instead of outputting that probability as a real number they actually make a probabilistic decision, and so what they actually output is either a one or a zero. They're intrinsically random.

So they're treating the P as the probability of producing a one, not as a real number. Of course, if the input is very big and positive they will almost always produce a one. If the input's big and negative they'll almost always produce a zero.

We can do a similar trick with rectified linear units. We can say that the output, there's real value that comes out of a rectified linear unit, if its input is above zero, is the rate of producing spikes. So that's deterministic. But once we've figured out these rate of producing spikes, the actual times at which spikes are produced is a random process. It's a Poisson process. So the rectified linear unit determines the rate, but intrinsic randomness in the unit determines when the spikes are actually produced.
\end{footnotesize}
\end{multicols}


%%%%%%%%%%
\subsection{A simple example of learning}


%%%%%%%%%%
\subsection{Three types of learning}
